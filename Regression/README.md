# Content
  - [Lecture 3 Regression æ‹·è²_å®Œæ•´.pdf](https://github.com/vanikk06/Machine-Learning/blob/master/Regression/Lecture%203%20Regression%20%E6%8B%B7%E8%B2%9D_%E5%AE%8C%E6%95%B4.pdf)
  - [Class_03.ipynb](https://github.com/vanikk06/Machine-Learning/blob/master/Regression/Class_03.ipynb)
    - [webpage](https://nbviewer.jupyter.org/github/vanikk06/Machine-Learning/blob/master/Regression/Class_03.ipynb)
  - [ä½œæ¥­2.pdf](https://github.com/vanikk06/Machine-Learning/blob/master/Regression/%E4%BD%9C%E6%A5%AD2.pdf)
    - [ä½œæ¥­2_answer.pdf](https://github.com/vanikk06/Machine-Learning/blob/master/Regression/%E4%BD%9C%E6%A5%AD2_answer.pdf)  
  - [Class_04.ipynb](https://github.com/vanikk06/Machine-Learning/blob/master/Regression/Class_04.ipynb)
    - [webpage](https://nbviewer.jupyter.org/github/vanikk06/Machine-Learning/blob/master/Regression/Class_04.ipynb)
  - [ä½œæ¥­3.pdf](https://github.com/vanikk06/Machine-Learning/blob/master/Regression/%E4%BD%9C%E6%A5%AD3.pdf)
    - [ä½œæ¥­ä¸‰.ipynb](https://github.com/vanikk06/Machine-Learning/blob/master/Regression/%E4%BD%9C%E6%A5%AD%E4%B8%89.ipynb)
      - [webpage](https://nbviewer.jupyter.org/github/vanikk06/Machine-Learning/blob/master/Regression/%E4%BD%9C%E6%A5%AD%E4%B8%89.ipynb)
    - [ä½œæ¥­3_answer.pdf](https://github.com/vanikk06/Machine-Learning/blob/master/Regression/%E4%BD%9C%E6%A5%AD3_answer.pdf)
  - [Correlation coefficient](https://github.com/vanikk06/Machine-Learning/tree/master/Regression#correlation-coefficient)
  - [Linear Regression](https://github.com/vanikk06/Machine-Learning/tree/master/Regression#linear-regression)
  - [Least square method ï¼† Gradient descent](https://github.com/vanikk06/Machine-Learning/tree/master/Regression#least-square-method--gradient-descent)
  - [Coefficient of Determination](https://github.com/vanikk06/Machine-Learning/tree/master/Regression#coefficient-of-determination)
  - [Pratices Linear Regression](https://github.com/vanikk06/Machine-Learning/tree/master/Regression#pratices-linear-regression)
  - [Overfitting v.s. Underfitting](https://github.com/vanikk06/Machine-Learning/tree/master/Regression#overfitting-vs-underfitting)
  - [Pratices Polynomial Regression](https://github.com/vanikk06/Machine-Learning/tree/master/Regression#pratices-polynomial-regression)
  - [Normal equation](https://github.com/vanikk06/Machine-Learning/tree/master/Regression#normal-equation)
  - [Pratices Multiple Regression](https://github.com/vanikk06/Machine-Learning/tree/master/Regression#pratices-multiple-regression)


# Correlation coefficient
  > ç›¸é—œä¿‚æ•¸


- Relationshipï¼šä»»å…©åˆ—è³‡æ–™æ¬„ï¼ˆå±¬æ€§ï¼‰ä¹‹é–“çš„é—œä¿‚
  > ç›¸é—œé—œä¿‚
  
- Correlation coefficientï¼šç›¸é—œä¿‚æ•¸ï¼Œç”¨ä»¥åæ˜ è®Šæ•¸ä¹‹é–“ç›¸é—œé—œä¿‚å¯†åˆ‡ç¨‹åº¦çš„çµ±è¨ˆæŒ‡æ¨™
  > ä»‹æ–¼\[-1, 1] ä¹‹é–“
  
  ![](https://github.com/vanikk06/Machine-Learning/blob/master/Regression/image/Snipaste_2020-02-13_03-54-34.png)
  
  æ˜¯é€éè¨ˆç®—ã€Œxyçš„æ¨™æº–å·®ã€é™¤ä»¥ã€Œxæ¨™æº–å·®ä¹˜ä»¥yæ¨™æº–å·®è€Œä¾†çš„ã€
  > ![](https://github.com/vanikk06/Machine-Learning/blob/master/Regression/image/Snipaste_2020-02-13_04-03-36.png)
  
  æ•¸å­—æ„ˆæ¥è¿‘ 1 ä»£è¡¨ç›¸é—œæ€§æ„ˆå¤§
    - æ­£ç›¸é—œï¼šæ­£è™Ÿ
    - è² ç›¸é—œï¼šè² è™Ÿ
    - ç›¸é—œä¿‚æ•¸=0ï¼šåƒ…ä»£è¡¨ä¸å­˜åœ¨ç·šæ€§ç›¸é—œï¼Œ**ä¸ä»£è¡¨ä¸å­˜åœ¨ç›¸é—œæ€§**

#### Source
[ç›¸é—œä¿‚æ•¸](https://wiki.mbalib.com/zh-tw/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0)

[âš—](https://github.com/vanikk06/Machine-Learning/tree/master/Regression#content)


# Linear Regression
  > ç·šæ€§å›æ­¸
  >> ç›®æ¨™è®Šæ•¸å¿…ç‚º**ã€Œé€£çºŒå€¼ã€**ï¼Œä¸å¯æ˜¯é¡åˆ¥å‹ã€é›¢æ•£å‹

ç·šæ€§è¿´æ­¸ï¼Œæ˜¯å°‹æ‰¾æ•¸æ“šèƒŒå¾Œéš±è—çš„å‡½å¼ï¼Œä¹Ÿå°±æ˜¯ç¬¦åˆæ•¸æ“šè¦å¾‹çš„å‡½å¼ï¼Œ
  > ç‚ºåœ¨è³‡æ–™é»ä¸­æ‰¾å‡ºè¦å¾‹ã€ç•«å‡ºä¸€æ¢ç›´ç·šçš„å°ˆæ¥­èªªæ³•
  
æ˜¯çµ±è¨ˆä¸Šåœ¨æ‰¾ã€Œå¤šå€‹è‡ªè®Šæ•¸å’Œä¾è®Šæ•¸ä¹‹é–“çš„é—œä¿‚ã€å»ºé€ å‡ºçš„æ¨¡å‹
  - ç°¡å–®ç·šæ€§å›æ­¸ï¼ˆSimple linear regressionï¼‰ï¼šä¸€å€‹è‡ªè®Šæ•¸èˆ‡ä¸€å€‹ä¾è®Šæ•¸
  - å¤šå…ƒå›æ­¸ï¼ˆmultiple regressionï¼‰ï¼šå¤§æ–¼ä¸€å€‹è‡ªè®Šæ•¸èˆ‡ä¸€å€‹ä¾è®Šæ•¸
      - è‡ªè®Šæ•¸ï¼ˆindependent variableï¼‰ï¼šç¨ç«‹ï¼Œç†è«–ä¸Šæ­¤è®Šæ•¸æ˜¯ä¸è¢«å…¶ä»–è®Šæ•¸å½±éŸ¿çš„ï¼Œåªæœƒå½±éŸ¿åˆ¥çš„è®Šæ•¸ï¼Œå› æ­¤è¢«èªç‚ºæ˜¯ã€Œå› ã€ï¼ˆCauseï¼‰
      
      - ä¾è®Šæ•¸ï¼ˆdependent variableï¼‰ï¼šç›¸ä¾ï¼Œæ­¤è®Šæ•¸åŸºæœ¬ä¸Šæ˜¯è¢«å…¶ä»–è®Šæ•¸å½±éŸ¿ï¼Œå¼•æ­¤è¢«èªç‚ºæ˜¯ã€Œæœã€ï¼ˆeffectï¼‰
  
  
  
ç·šæ€§æ¨¡å‹ï¼šé»èˆ‡ç·šä¹‹é–“è·é›¢æœ€çŸ­
 > ç·šæ€§å‡½æ•¸ï¼šy = ax + b
 >> ![](https://github.com/vanikk06/Machine-Learning/blob/master/Regression/image/Snipaste_2020-02-13_04-14-05.png)
 
- Machine learningï¼šåˆ©ç”¨ data è¨ˆç®—å‡è¨­ gï¼Œè®“å…¶è¿‘ä¼¼ç›®æ¨™ f
  - Step 1. æ”¶é›†è¨“ç·´è³‡æ–™
  - Step 2. è¨­è¨ˆæ•¸å­¸æ¨¡å‹
  - Step 3. æ‰¾å‡ºå…·æœ€ä½³åƒæ•¸ä¹‹æ¨¡å‹


#### Source
[ç·šæ€§è¿´æ­¸çš„é‹ä½œåŸç†](https://brohrer.mcknote.com/zh-Hant/how_machine_learning_works/how_linear_regression_works.html)

[ç·šæ€§å›æ­¸(Linear Regression)](https://medium.com/@chih.sheng.huang821/%E7%B7%9A%E6%80%A7%E5%9B%9E%E6%AD%B8-linear-regression-3a271a7453e)

[ğŸ§ª](https://github.com/vanikk06/Machine-Learning/tree/master/Regression#content)

# Least square method ï¼† Gradient descent
  > ã€Œæœ€å°å¹³æ–¹æ³•ã€èˆ‡ã€Œæ¢¯åº¦ä¸‹é™æ³•ã€

åˆ©ç”¨ã€Œæœ€å°å¹³æ–¹æ³•ã€èˆ‡ã€Œæ¢¯åº¦ä¸‹é™æ³•ã€æ‰¾å‡ºæœ€ä½³çš„å›æ­¸ç·š
- Least square methodï¼šç›®çš„æ˜¯ç‚ºäº†å°‹æ‰¾**æœ€å°**çš„cost functionï¼Œå¸Œæœ›èª¤å·®çš„å¹³æ–¹æ„ˆå°æ„ˆå¥½ï¼Œå› èª¤æ’æœ‰æ­£æœ‰è² ï¼Œå–å¹³æ–¹å¾Œçš†ç‚ºæ­£å€¼

  > P.S. è·é›¢çš„è¨ˆç®—ã€Œå¹³æ–¹ã€æœƒå„ªæ–¼ã€Œçµ•å°å€¼ã€
  >> å› ç‚ºåœ¨å°‹æ‰¾ã€Œæœ€ä½³åŒ–ã€å‡½æ•¸æ™‚ï¼Œæœƒä½¿ç”¨å¾®ç©åˆ†çš„æ–¹å¼ï¼Œçµ•å°å€¼å¾®åˆ†çš„è©±ï¼Œåœ¨0æœƒçªç„¶è®ŠåŒ–åŠ‡çƒˆ
  ï¼ˆsharpï¼‰ï¼Œä¸åˆ©æ–¼åˆ¤æ–·
  
  - Cost function / Loss functionï¼šæå¤±å‡½æ•¸ï¼Œåœ¨æœ€ä½³åŒ–ç†è«–è£¡ç¨±ç‚ºç›®æ¨™å‡½æ•¸ï¼ˆobjection functionï¼‰ï¼Œå¸Œæœ›ç›®æ¨™å‡½æ•¸æ„ˆå°æ„ˆå¥½
    > æ‰¾ minimize
    
    ![](https://github.com/vanikk06/Machine-Learning/blob/master/Regression/image/Snipaste_2020-02-14_00-02-57.png)
    > å¾®åˆ†æ™‚ï¼Œå‡½å¼æœƒæˆå€æ•¸æˆé•·ï¼Œå› æ­¤ã€Œä¹˜ä¸Š1/2ã€å¯ä½¿å‡½å¼æ›´ç‚ºç°¡æ½”ï¼Œä¸”ä¸å½±éŸ¿æœ€å°å€¼
   
  
- Gradient descentï¼šé€éä¸æ–·**æ›´æ–°åƒæ•¸**æ…¢æ…¢é è¿‘ç›®æ¨™ï¼Œæœ€çµ‚æ‰¾åˆ°ä¸€å€‹æ¥µè¿‘ä¼¼ç›®æ¨™çš„å‡½å¼
  > å¾®ç©åˆ†
   
   æ¢¯åº¦ä¸‹é™æ³•ï¼Œæ˜¯æœ€ä½³åŒ–ç†è«–è£¡çš„ä¸€å€‹ä¸€éšæ‰¾æœ€ä½³è§£çš„ä¸€ç¨®æ–¹æ³•ï¼Œåˆ©ç”¨æ­¤æ–¹æ³•æ‰¾åˆ°å‡½å¼ä¸­å±€éƒ¨æœ€å°å€¼
   > æ‰¾ä¸€çµ„Î¸<sub>0</sub>, Î¸<sub>1</sub> ä½¿E(Î¸<sub>0</sub>, Î¸<sub>1</sub>)æœ€å°
   >> Î¸<sub>1</sub>ï¼šæ–œç‡ï¼Œèˆ‡ç›¸é—œä¿‚æ•¸åŒè™Ÿ\
   >> Î¸<sub>0</sub>ï¼šæˆªè·ï¼ˆé«˜åº¦ï¼‰
   >>> å¾\[0,0]é–‹å§‹æ…¢æ…¢æ‰¾ï¼Œç”¨æ¯é»èˆ‡å›æ­¸ç·šçš„è·é›¢ç¸½åˆç‚ºåˆ¤æ–·ï¼Œæ‰¾æœ€å°å€¼
   
   - æ¢¯åº¦
     > å¾®åˆ†ï¼šå‡½å¼åœ¨æŸé»ä¸Šè®ŠåŒ–çš„æ–¹å‘
        - ä¸€ç¶­åº¦çš„ç´”é‡xçš„æ¢¯åº¦ï¼šè¨ˆç®—f(x)å°xçš„å¾®åˆ†
        - å¤šç¶­åº¦çš„å‘é‡xçš„æ¢¯åº¦ï¼šè¨ˆç®—f(x)å°xæ‰€æœ‰å…ƒç´ çš„åå¾®åˆ†ï¼Œå³å°æ¯å€‹å…ƒç´ åšåå¾®åˆ†æ‰€çµ„æˆçš„å‘é‡ç©ºé–“
        
      å¯ä»¥å°‡æ¢¯åº¦æƒ³åƒæˆä¸€å€‹æŒ‡å‘æœ€ä½é»çš„æŒ‡å—é‡ï¼Œåœ¨æ¢¯åº¦ä¸‹é™æ³•ä¸­æˆ‘å€‘æ˜¯å¾€æ¢¯åº¦çš„åæ–¹å‘èµ°
      
    åœ¨å‡½å¼ä¸­å°‹æ‰¾æ¥µå°å€¼ï¼Œå¯ä»¥æƒ³åƒæ˜¯åœ¨ä¸€å€‹æ‹‹ç‰©ç·šä¸­å°‹æ‰¾æœ€ä½é»ï¼Œæ­é…ä½åº¦ä¸‹é™æ³•
     - ç•¶ f'(x) < 0ï¼Œè¡¨ç¤ºç›®å‰æ‰€åœ¨ç‚ºè² æ–œç‡çš„ç·šï¼Œå› æ­¤è¦å¾€å³ç§»å‹•ï¼Œæ‰æœƒæ¥è¿‘æœ€ä½é»
     - ç•¶ f'(x) > 0ï¼Œè¡¨ç¤ºç›®å‰æ‰€åœ¨ç‚ºæ­£æ–œç‡çš„ç·šï¼Œå› æ­¤è¦å¾€å·¦ç§»å‹•ï¼Œæ‰æœƒæ¥è¿‘æœ€ä½é»
    
    é€éä¸Šè¿°ç§»å‹•Î¸ï¼Œè®“å…¶å¾€èˆ‡å°å‡½æ•¸ï¼ˆf'(x)ï¼‰ç›¸åçš„æ–¹å‘ç§»å‹•ï¼Œå°±æœƒå¾€æœ€å°å€¼çš„ä½ç½®ç§»å‹•
    
    1. f'(x)æ±ºå®šæ–¹å‘
    2. Î·ï¼šlearning rateï¼Œæ¯æ¬¡ç§»å‹•çš„å¤§å°
       > modelè¦è¨­å®šçš„åƒæ•¸
       >> è¨­å®šä¸å¥½æœƒç„¡æ³•æ”¶æ–‚åˆ°æœ€å°
       
       ![](https://github.com/vanikk06/Machine-Learning/blob/master/Regression/image/Snipaste_2020-02-14_02-26-43.png)
       ![](https://github.com/vanikk06/Machine-Learning/blob/master/Regression/image/Snipaste_2020-02-14_02-36-50.png)
       ![](https://github.com/vanikk06/Machine-Learning/blob/master/Regression/image/Snipaste_2020-02-14_02-38-07.png)
       

#### Â§ Pratices Â§
[ğŸ‘‰ğŸ»HEREğŸ‘ˆğŸ»](https://github.com/vanikk06/Machine-Learning/blob/master/Regression/Class_03.ipynb)

- ç›´ç·š

  ![](https://github.com/vanikk06/Machine-Learning/blob/master/Regression/image/Snipaste_2020-02-14_03-02-20.png)

  - h<sub>Î¸</sub>( x ) = Î¸<sub>0</sub> + Î¸<sub>1</sub>x
  - Î¸<sub>0</sub>èª¿æ•´ï¼šh<sub>Î¸</sub>( x ) - y
  - Î¸<sub>1</sub>èª¿æ•´ï¼š( h<sub>Î¸</sub>( x ) - y )*x
  
- äºŒé …å¼

  ![](https://github.com/vanikk06/Machine-Learning/blob/master/Regression/image/Snipaste_2020-02-15_02-27-46.png)
  
  - h<sub>Î¸</sub>( x ) = Î¸<sub>0</sub> + Î¸<sub>1</sub>x + Î¸<sub>2</sub>x<sup>2</sup>
  - Î¸<sub>0</sub>èª¿æ•´ï¼šh<sub>Î¸</sub>( x ) - y
  - Î¸<sub>1</sub>èª¿æ•´ï¼š( h<sub>Î¸</sub>( x ) - y )x
  - Î¸<sub>2</sub>èª¿æ•´ï¼š( h<sub>Î¸</sub>( x ) - y )x<sup>2</sup>
  

æ¢¯åº¦ä¸‹é™æ³•æ˜¯ä¸æ–·å¾€ä¸‹æ‰¾èª¤å·®ï¼ˆcostï¼‰çš„æœ€å°å€¼ï¼Œä½†åœ¨æ‰¾åˆ°ä¹‹å‰ï¼Œæˆ‘å€‘ä¸çŸ¥é“è¦çµ¦modelç–Šä»£å¤šå°‘æ¬¡æ‰æœƒæ‰¾åˆ°ï¼Œå› æ­¤å¯ä»¥åœ¨modelä¸Šã€Œçµ¦äºˆèª¤å·®è‡¨ç•Œå€¼çš„é™åˆ¶ã€ï¼Œè®“modelåœ¨èª¤å·®çš„è®ŠåŒ–å°åˆ°ä¸€å®šç¨‹åº¦æ™‚ï¼Œå°±çŸ¥é“è¦åœæ­¢ï¼Œä»¥æ¸›å°‘å¤šé¤˜çš„æ™‚é–“ã€è¨˜æ†¶é«”æµªè²»
   
      
P.S. è¦å°å¿ƒï¼Œæ‰¾åˆ°çš„ min æ˜¯ local é‚„æ˜¯ global
      
#### Source
[[æ·±åº¦å­¸ç¿’è¬›ä¸­æ–‡] ç°¡å–®è§£é‡‹æ¢¯åº¦ä¸‹é™æ³• (Gradient Descent)](https://medium.com/@arlen.mg.lu/%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92%E8%AC%9B%E4%B8%AD%E6%96%87-gradient-descent-b2a658815c72)

[æ©Ÿå™¨/æ·±åº¦å­¸ç¿’-åŸºç¤æ•¸å­¸(äºŒ):æ¢¯åº¦ä¸‹é™æ³•(gradient descent)](https://medium.com/@chih.sheng.huang821/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E5%9F%BA%E7%A4%8E%E6%95%B8%E5%AD%B8-%E4%BA%8C-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-gradient-descent-406e1fd001f)

[[Day4] æ¢¯åº¦ä¸‹é™æ³•ï¼ˆGradient Descentï¼‰](https://ithelp.ithome.com.tw/articles/10193297?sc=iThelpR)

[ç·šæ€§å›æ­¸(Linear Regression)](https://medium.com/@chih.sheng.huang821/%E7%B7%9A%E6%80%A7%E5%9B%9E%E6%AD%B8-linear-regression-3a271a7453e)

[ğŸ§«](https://github.com/vanikk06/Machine-Learning/tree/master/Regression#content)

# Coefficient of Determination
  > æ±ºå®šä¿‚æ•¸
  
ç•¶modelè¨“ç·´å¥½å¾Œï¼Œè¦è©•ä¼°modelçš„æ•ˆèƒ½å¦‚ä½•

åœ¨æ­¤ linear regression modelä¸»è¦æ˜¯è¦å°‹æ‰¾ã€Œèª¤å·®æ¥µå°å€¼ã€ï¼Œå› æ­¤å¯ä»¥ç”¨ MSE ä½œè©•ä¼°
- MSEï¼ˆMean Squared Errorï¼‰ï¼šå‡æ–¹èª¤å·®ï¼Œå¯å°‡å…¶è¦–ç‚ºCost function
  > è¿´æ­¸modelçš„æ ¸å¿ƒï¼Œæ‰®æ¼”å°‹æ‰¾ä¿‚æ•¸çš„ä¾æ“š
  >> å–®ç¨è§£è®€ç„¡æ³•åˆ¤æ–·å¥½å£ï¼Œéœ€é€éæ¯”è¼ƒæ‰æœ‰æ„ç¾©
 
  
  ![](https://github.com/vanikk06/Machine-Learning/blob/master/Regression/image/Snipaste_2020-02-15_02-58-23.png)
  > MSEæ„ˆå°ï¼Œä»£è¡¨é æ¸¬å€¼èˆ‡å¯¦éš›å€¼å·®è·æ„ˆå°
  
  åœ¨ç·šæ€§è¿´æ­¸ä¸­
  
  ![](https://github.com/vanikk06/Machine-Learning/blob/master/Regression/image/Snipaste_2020-02-15_03-03-51.png)
  
  ![](https://github.com/vanikk06/Machine-Learning/blob/master/Regression/image/Snipaste_2020-02-15_03-08-05.png)
  
  - SSTï¼šç¸½èª¤å·®ï¼Œç‚º çœŸå¯¦å€¼_y åˆ° å¹³å‡å€¼_y çš„è·é›¢ï¼Œå¯åˆ†ç‚º SSE èˆ‡ SSR
  - SSRï¼šè¿´æ­¸modelå¯è§£é‡‹çš„éƒ¨åˆ†ï¼Œç‚º é æ¸¬å€¼_y åˆ° å¹³å‡å€¼_y çš„è·é›¢  
  - SSEï¼šéš¨æ©Ÿèª¤å·®ï¼Œç‚º çœŸå¯¦å€¼_y åˆ° é æ¸¬å€¼_y çš„è·é›¢
  
  SST = SSR + SSE\
  é€éä¸Šè¿°å¼å­ï¼Œå¯å°‡ SSR æ–¼ SST ä¸­çš„ä½”æ¯”ï¼Œç†è§£ç‚ºä¸€å€‹ã€Œæ¨™æº–åŒ–å¾Œçš„MSEã€ï¼Œå‰‡ç‚ºR<sup>2</sup>
  > å› å…¶ç¸®è‡³\[0, 1]
  
  R<sup>2</sup>æ„ˆé«˜ï¼Œè¡¨ç¤º model çš„è§£é‡‹èƒ½åŠ›æ„ˆå¥½
  - R<sup>2</sup> = 1ï¼šè¡¨ç¤º model å®Œå…¨è§£é‡‹æ•¸æ“š
  - R<sup>2</sup> = 0.7 ~ 0.8ï¼šè¡¨ç¤º model éƒ¨åˆ†è§£é‡‹æ•¸æ“š
  - R<sup>2</sup> = 0 ï¼šè¡¨ç¤º model å®Œå…¨ç„¡æ³•è§£é‡‹æ•¸æ“š
  
#### Source 
[æ©Ÿå™¨å­¸ç¿’-ç·šæ€§å›æ­¸åˆ†æ(linear regression)](http://www.taroballz.com/2018/07/16/ML_LinearRegression/)

[å¦‚ä½•é æ¸¬è³‡æ–™ï¼šè¿´æ­¸æ¨¡å‹çš„è©•ä¼°](https://medium.com/datainpoint/evaluating-reg-e993ce27b61)

[ğŸ’‰](https://github.com/vanikk06/Machine-Learning/tree/master/Regression#content)


# Pratices Linear Regression
  > ç”¨å¥—ä»¶ä¾†å¯¦è¸ Linear Regression

è®“æ©Ÿå™¨é€éå­¸ç¿’è§€å¯Ÿç‰¹å¾µä¾†åˆ†é¡

å…ˆåŒ¯å…¥ç›¸é—œå¥—ä»¶èˆ‡è³‡æ–™ï¼Œä¸¦å°‡è³‡æ–™ç”±dataframeè½‰ç‚ºarray

```python
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

data = pd.read_csv('regression1.csv')
x = data.iloc[:,0].values
y = data.iloc[:,-1].values
```

å…ˆå°‡è³‡æ–™ç‰¹å¾µé€²è¡Œ normalization è™•ç†ï¼Œè®“ç‰¹å¾µè³‡æ–™ï¼ˆXï¼‰ä¸æ”¹è®ŠåŸå§‹åˆ†ä½ˆçš„æŒ‰æ¯”ä¾‹ç¸®æ”¾ï¼Œè®“è³‡æ–™è½åœ¨æŸä¸€ç‰¹å®šå€é–“
> ä½¿ç”¨`sklearn.preprocessing`å¥—é–“ä¸­çš„`StandardScaler`å‡½å¼
```python
from sklearn.preprocessing import StandardScaler

sc_x = StandardScaler()
x1 = x.reshape(-1,1)

X_std = sc_x.fit_transform(x1)
```
> å°å¿ƒï¼Œfit_transform()çš„inputæ ¼å¼ç‚º2-D array

- `StandardScaler()`ï¼šæ¨™æº–åŒ–ï¼Œèƒ½å¤ è¨ˆç®—è¨“ç·´æ•¸æ“šçš„å¹³å‡å€¼å’Œæ¨™æº–å·®ï¼Œå¾è€Œåœ¨è¨“ç·´æ•¸æ“šé›†ä¸Šå†æ¬¡ä½¿ç”¨
  > å¹³å‡å€¼ç‚º 0ï¼›å–®ä½ç‚ºæ¨™æº–å·®
  
  - æ¨™æº–åŒ–ï¼šåœ¨æ­¤ç‚ºã€Œæ¸›æ‰å¹³å‡å€¼ï¼Œå†é™¤ä»¥æ¨™æº–å·®ã€ï¼Œæœ‰å°±æ˜¯å»é™¤å‡å€¼ï¼Œå†æŒ‰æ¨™æº–å·®æ¯”ä¾‹ç¸®æ”¾

- `np.reshape(newshape)`ï¼šæ”¹è®Šarrayå½¢ç‹€ï¼Œä¸æ”¹è®Šå…§éƒ¨å…ƒç´ 
  - newshapeï¼šæ–°å½¢ç‹€
    - intï¼š1-D array
    - int of intsï¼š(row, col)
      > æ–°å½¢ç‹€çš„å…ƒç´ å€‹æ•¸ï¼Œè¦ç­‰æ–¼åŸarrayçš„å…ƒç´ å€‹æ•¸
      >> -1ï¼šè‡ªå‹•è¨ˆç®—

- `sc_x.fit_transform()`ï¼šåŸ·è¡Œè¨“ç·´ä¸¦è½‰åŒ–
  > inputï¼š2-D array

æ¥è‘—å†å»ºç«‹ç·šæ€§è¿´æ­¸model
> ä½¿ç”¨`sklearn.linear_model`å¥—ä»¶ä¸­çš„`LinearRegression`å‡½å¼
```python
from sklearn.linear_model import LinearRegression

lr = LinearRegression()
lr.fit(X_std, y)
y_pred = lr.predict(X_std)
```
- `LinearRegression()`ï¼šå¯¦ä¾‹åŒ–
- `lr.fit(X_std, y)`ï¼šè¨“ç·´/æ“¬åˆmodel
    - X_stdï¼šç‰¹å¾µè®Šæ•¸
    - yï¼šç›®æ¨™è®Šæ•¸
- lr.predict(X_std)ï¼šå°‡è³‡æ–™æ”¾å…¥modelä¸­é€²è¡Œé æ¸¬


æŸ¥çœ‹è¨“ç·´å‡ºçš„è¿´æ­¸ç·šä¿‚æ•¸
```python
print('Slope: %.3f' % lr.coef_[0])
print('Intercept: %.3f' % lr.intercept_)
#è¼¸å‡º
Slope: 95.564
Intercept: 428.600
```
- `lr.coef_[0]`ï¼šæ–œç‡ï¼ˆÎ¸<sub>1</sub>ï¼‰
- `lr.intercept_`ï¼šæˆªè·ï¼ˆÎ¸<sub>0</sub>ï¼‰
    - `%.3f`ï¼šå­—ä¸²åŒ–æ ¼å¼
      - %ï¼šæ“ä½œç¬¦ï¼Œå¯å°‡è³‡æ–™æ ¼å¼åŒ–
        > åŸºæœ¬èªæ³•`'' % ('')`
        >> [Learning more](https://www.footmark.info/programming-language/python/python-string-format/)
      - fï¼šæµ®é»æ•¸
      - .3ï¼šå°æ•¸é»å¾Œä¸‰ä½

ç•«å‡ºè³‡æ–™çš„æ•£ä½ˆåœ–èˆ‡è¿´æ­¸ç·šä¾†çœ‹å½¼æ­¤çš„é—œä¿‚
```python
plt.scatter(X_std, y)
plt.plot(X_std, y_pred, 'c')
```
ç”¨ MSE èˆ‡ R<sup>2</sup> ä¾†è©•ä¼°model
> ä½¿ç”¨`sklearn.metrics`å¥—ä»¶
```python
import sklearn.metrics as sm

print('MSE: %.3f' % sm.mean_squared_error(y, y_pred))
print('R^2: %.3f' % sm.r2_score(y, y_pred))
#è¼¸å‡º
MSE: 978.262
R^2: 0.903
```
> model åªèƒ½è§£é‡‹90.3%çš„éŠ·å”®æ”¶å…¥è®Šå› 

- `sm.mean_squared_error(y, y_pred)`ï¼šå‡æ–¹èª¤å·®è¿´æ­¸æå¤±ï¼Œæ‹¿ modelé æ¸¬å€¼ èˆ‡ çœŸå¯¦å€¼ æ¯”è¼ƒ
    - yï¼šçœŸå¯¦æ•¸æ“šçš„ç›®æ¨™è®Šæ•¸
    - y_predï¼šmodelçš„é æ¸¬å€¼
- `sm.r2_score(y, y_pred)`ï¼šæ¸¬é‡ x å° y çš„è§£é‡‹ç¨‹åº¦ï¼Œä¹Ÿå°±æ˜¯é æ¸¬å€¼ y çš„è®Šç•°ä¸­ï¼Œæœ‰å¤šå°‘ç™¾åˆ†æ¯”å¯ä»¥ç”± x è§£é‡‹
  > æœ€ä½³ç‚º 1\
  > æœ‰å¯èƒ½ç‚ºè² 

#### Source
[sklearnä¸­çš„æ•°æ®é¢„å¤„ç†](http://d0evi1.com/sklearn/preprocessing/)

[Day13-Scikit-learnä»‹ç´¹(5)_ Linear-Regression](https://ithelp.ithome.com.tw/articles/10206114)

[Python å­—ä¸²æ ¼å¼åŒ–](https://www.footmark.info/programming-language/python/python-string-format/)

[ğŸ§¬](https://github.com/vanikk06/Machine-Learning/tree/master/Regression#content)


# Overfitting v.s. Underfitting

modelä¸»è¦åˆ©ç”¨training setè¨“ç·´ï¼Œä»¥æœŸå¾…å¯ä»¥ä½¿ç”¨æ­·å²æ•¸æ“šï¼Œé€²è¡Œé æ¸¬

- Overfittingï¼šmodel éåº¦æ“¬åˆè¨“ç·´é›†çš„ç‰¹å¾µï¼Œå°è‡´ model å¤±å»æ³›åŒ–èƒ½åŠ›

- Underfittingï¼šmodel æ“¬åˆä¸è¶³ï¼Œmodel æ²’æœ‰å­¸å¥½è¨“ç·´é›†çš„ç‰¹å¾µï¼Œå°è‡´ model ç„¡æ³•é€²è¡Œé æ¸¬

#### å¦‚ä½•åˆ¤æ–·Overfitting??
å¯å°‡ data set åˆ†å‰²ç‚º training set èˆ‡ testing set
1. training setï¼šå»ºç«‹model
2. testing setï¼šé æ¸¬ï¼Œä»¥è©•ä¼°model

- Overfitting ç™¼ç”Ÿæƒ…å½¢
  - train MSE < test MSE
    > MSEæ„ˆå°æ„ˆå¥½
  - train R<sup>2</sup> > test R<sup>2</sup>
    > R<sup>2</sup>æ„ˆæ¥è¿‘ 1 æ„ˆå¥½


[ğŸ’Š](https://github.com/vanikk06/Machine-Learning/tree/master/Regression#content)

#  Pratices Polynomial Regression
   > äºŒé …å¼ç·šæ€§å›æ­¸
   >> é€šéå¢åŠ  x çš„é«˜æ¬¡é …ï¼Œå°æ•¸æ“šé€²è¡Œé€¼è¿‘

- Polynomial Regressionï¼šæ•¸æ“šåˆ†ä½ˆç‚ºæ›²ç·šï¼Œè€Œéç›´ç·š

è¦é€²è¡Œå¤šé …å¼å‡½æ•¸çš„ç·šæ€§è¿´æ­¸å‰ï¼Œè¦å…ˆå°ç‰¹å¾µè®Šæ•¸åšäº›è™•ç†ï¼Œè®“å…¶å¢åŠ äºŒæ¬¡é …

ä½¿ç”¨`sklearn.preprocessing`å¥—ä»¶çš„`PolynomialFeatures`å‡½å¼
```python
from sklearn.preprocessing import PolynomialFeatures

quadratic = PolynomialFeatures(degree=2)
x_quad = quadratic.fit_transform(X_std)
```
- `PolynomialFeatures(degree=2, interaction_only=False)`ï¼šå¯¦ä¾‹åŒ–ï¼Œå°ˆé–€ç”Ÿæˆã€Œå¤šé …å¼ç‰¹å¾µã€ï¼Œä¸¦ä¸”å¤šé …å¼åŒ…å«çš„æ˜¯ç›¸äº’å½±éŸ¿çš„ç‰¹å¾µé›†
   > è¦æŒ‡å®šã€Œå‡½å¼ç¶­åº¦ã€
   >> [Learning more](https://www.itread01.com/content/1541737027.html)
   - degreeï¼šå¤šé …å¼éšæ•¸ï¼Œé è¨­ç‚º2
   - interaction_onlyï¼šæ˜¯å¦åªæ‰¾äº’å‹•ä½œç”¨çš„å¤šé …å¼
     > é è¨­False
     >> äº’å‹•ä½œç”¨ï¼šèˆ‡ä»–è€…äº’å‹•\
     - interaction_only = Trueï¼š\[1, a, b, a<sup>2</sup>, ab, b<sup>2</sup>] æœƒè®Šæˆåƒ…æœ‰ \[1, a, b, ab]
       > ç„¡è‡ªå·±èˆ‡è‡ªå·±äº’å‹•çš„æƒ…å½¢
- `quadratic.fit_transform(X_std)`ï¼šå°‡æ•¸æ“šæ”¾å…¥modelè½‰æ›
  > å¯è½‰æ›ç‚º x å…§ç© Î¸
  >> åœ¨æ­¤ç‚º h<sub>Î¸</sub>( x ) = Î¸<sub>0</sub> + Î¸<sub>1</sub>x + Î¸<sub>2</sub>x<sup>2</sup> ç‚º ä¸‰ç¶­å‘é‡
  - inputï¼š2-D array
  
  ç”¢ç”Ÿä¸€å€‹äºŒé …å¼çš„ä¿‚æ•¸
  ```python
  x = np.arange(4)
  x = x.reshape(-1,1)
  x
  #è¼¸å‡º
  array([[0],
         [1],
         [2],
         [3]])
  ```
  å°‡è³‡æ–™æ ¼å¼æ¯rowç‚º \[a] çš„ x æ”¾å…¥
  ```python
  poly = PolynomialFeatures(2)
  poly.fit_transform(x)
  #è¼¸å‡º
  array([[1., 0., 0.],
         [1., 1., 1.],
         [1., 2., 4.],
         [1., 3., 9.]])
  ```
  > æœƒç”Ÿæˆ \[1, a, a<sup>2</sup>]çš„è³‡æ–™
  
  æ”¹æˆç”Ÿæˆä¸‰é …å¼çš„ä¿‚æ•¸
  ```python
  poly = PolynomialFeatures(3)
  poly.fit_transform(x) 
  #è¼¸å‡º
  array([[ 1.,  0.,  0.,  0.],
         [ 1.,  1.,  1.,  1.],
         [ 1.,  2.,  4.,  8.],
         [ 1.,  3.,  9., 27.]])
  ```
  > ç”Ÿæˆ \[1, a, a<sup>2</sup>, a<sup>3</sup>]
  
  å°‡æ¯rowçš„è³‡æ–™æ ¼å¼æ›´æ”¹ \[a, b]
  ```python
  y = np.arange(4).reshape(2,2)
  y
  #è¼¸å‡º
  array([[0, 1],
         [2, 3]])
  ```
  ```python
  poly = PolynomialFeatures(2)
  poly.fit_transform(y)
  #è¼¸å‡º
  array([[1., 0., 1., 0., 0., 1.],
         [1., 2., 3., 4., 6., 9.]])
  ```
  > ç”Ÿæˆ \[1, a, b, a<sup>2</sup>, ab, b<sup>2</sup>]
  
  æ›´æ”¹åƒæ•¸ï¼Œåƒ…æœ‰ä»–è€…äº’å‹•ä½œç”¨çš„åƒæ•¸
  ```python
  poly = PolynomialFeatures(2, interaction_only=True)
  poly.fit_transform(y)
  #è¼¸å‡º
  array([[1., 0., 1., 0.],
         [1., 2., 3., 6.]])
  ```
  > ç”Ÿæˆ \[1, a, b, ab]
  >> ä¸å­˜åœ¨è‡ªå·±èˆ‡è‡ªå·±äº’å‹•çš„æƒ…å½¢\
  >> E.g. a<sup>2</sup>ã€b<sup>2</sup>
  
  
æ”¾å…¥ç·šæ€§è¿´æ­¸æ¨¡å‹åšè¨“ç·´
```python
pr = LinearRegression()
pr.fit(x_quad, y)
y_quad_pred = pr.predict(x_quad)

print('theta1: %.3f' % pr.coef_[1])
print('theta2: %.3f' % pr.coef_[2])
print('Intercept: %.3f' % pr.intercept_)
#è¼¸å‡º
theta1: 97.133
theta2: 22.623
Intercept: 405.977
```
- `pr.coef_[1]`ï¼šxä¿‚æ•¸
- `pr.coef_[2]`ï¼šx<sup>2</sup>ä¿‚æ•¸
  > pr.coef_\[0]ï¼šé è¨­ç‚º1ï¼Œå› å…¶å¯é‚„åŸç‚º x å…§ç© Î¸
- `pr.intercept_`ï¼šæˆªè·


è©•ä¼°æ¨¡å‹
```python
print('MSE: %.3f' % sm.mean_squared_error(y, y_quad_pred))
print('R^2: %.3f' % sm.r2_score(y, y_quad_pred))
#è¼¸å‡º
MSE: 377.582
R^2: 0.963
```

#### Source
[Day13-Scikit-learnä»‹ç´¹(5)_ Linear-Regression](https://ithelp.ithome.com.tw/articles/10206114)

[Sklearn-preprocessing.PolynomialFeatures](https://www.itread01.com/content/1541737027.html)

[ğŸ”¬](https://github.com/vanikk06/Machine-Learning/tree/master/Regression#content)


# Normal equation
  > æ­£è¦æ–¹ç¨‹å¼
  >> cost functionè½‰ç‚ºçŸ©é™£ï¼Œå°çŸ©é™£å¾®åˆ†ï¼Œæ¨å€’æœ€å°å€¼
  
åœ¨æ©Ÿå™¨å­¸ç¿’ä¸­ï¼Œå¦‚æœæƒ³è¦æ±‚å¾—ç·šæ€§è¿´æ­¸çš„åƒæ•¸ï¼Œå¯ä»¥ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•ï¼ˆGradient Descentï¼‰æˆ–æ­£è¦æ–¹ç¨‹å¼ï¼ˆNormal Equationï¼‰ï¼Œä¾†å°‹æ‰¾è®“ Cost Function é”åˆ°æœ€å°å€¼çš„æœ€ä½³åƒæ•¸

å¤šå…ƒç·šæ€§è¿´æ­¸å¯ç”¨ä¸‹åˆ—å¼å­ç”¨å…©è®Šæ•¸ä¹‹é–“çš„é—œä¿‚è¡¨ç¤ºï¼Œå¯è¦–ç‚ºå…©é …é‡å…§ç©
![](https://github.com/vanikk06/Machine-Learning/blob/master/Regression/image/Snipaste_2020-02-17_03-08-32.png)  
 
Normal equationæ˜¯å°‡ cost function è½‰æˆçŸ©é™£çš„å½¢å¼ï¼Œé€éå°å…¶åšä¸€éšåå¾®åˆ†ï¼Œæ‰¾cost functionçŸ©é™£æœ€å°å€¼æ™‚çš„è¿´æ­¸ä¿‚æ•¸

#### Gradient descent v.s. Normal equation
  
- æ˜¯å¦è¨­å®š learning rateï¼ˆå­¸ç¿’ç‡ï¼‰
  - Gradient descentï¼šéœ€è¨­å®šå­¸ç¿’ç‡ï¼Œå­¸ç¿’ç‡é¸æ“‡çš„éç¨‹é€šå¸¸éœ€è¦è·‘æ•¸æ¬¡è³‡æ–™ä¾†æ±ºå®šï¼ˆéœ€è¦å¤šæ¬¡çš„ç–Šä»£ï¼‰
    > å­¸ç¿’ç‡çš„å¤§å°æœƒå½±éŸ¿åˆ°ä¸‹é™çš„æ”¶æ–‚é€Ÿåº¦ï¼Œè‹¥é¸æ“‡å¤±ç•¶ï¼Œæœ‰å¯èƒ½é€ æˆç™¼æ•£
  - Normal equationï¼šä¸é ˆè¨­å®šå­¸ç¿’ç‡ï¼Œä¹Ÿä¸é ˆç–Šä»£é‹ç®—
  
- Normal equation éœ€è¦è¨ˆç®—ã€ŒåçŸ©é™£ã€ï¼Œç•¶è‡ªè®Šæ•¸çš„æ•¸ç›®éå¸¸å¤šæ™‚ï¼ŒçŸ©é™£çš„ç¶­åº¦æœƒè®Šå¤§ï¼Œå°è‡´æ™‚é–“è¤‡é›œåº¦å¢åŠ ï¼Œé€™æ™‚ä½¿ç”¨ Gradient descent æœƒç›¸å°å¿«é€Ÿ
  > ç•¶è‡ªè®Šæ•¸ä¸å¤šæ™‚ï¼Œä½¿ç”¨ Normal equation æ˜¯ç›¸å°ç¶“æ¿Ÿçš„é¸æ“‡
 
#### Source
[Normal Equationï¼ˆæ­£è¦æ–¹ç¨‹å¼ï¼‰](https://medium.com/@gatorsquare/ml-normal-equation-%E6%AD%A3%E8%A6%8F%E6%96%B9%E7%A8%8B%E5%BC%8F-9f1c09de4217)

[âš–](https://github.com/vanikk06/Machine-Learning/tree/master/Regression#content)

# Pratices Multiple Regression
  > å¤šå…ƒè¿´æ­¸ã€è¤‡è¿´æ­¸

ç‰¹å¾µè®Šæ•¸ç‚ºä¸€å€‹ä»¥ä¸Š

è¼‰å…¥ç³–å°¿ç—…æ•¸æ“šé›†
```python
from sklearn.datasets import load_diabetes

data = load_diabetes()
data.keys()
#è¼¸å‡º
dict_keys(['data', 'target', 'DESCR', 'feature_names', 'data_filename', 'target_filename'])
```
- `load_diabetes()`ï¼šå¯¦åŠ›åŒ–è³‡æ–™é›†
   - åå€‹ç‰¹å¾µè®Šæ•¸ï¼ˆå·²æ¨™æº–åŒ–ï¼‰
     - Age
     - Sex
     - Body mass indexï¼ˆBMIï¼‰
     - Average blood pressureï¼šå¹³å‡è¡€å£“
     - S1 ~ S6ï¼š6ç¨®ç”Ÿç†æ•¸æ“š
   - ç›®æ¨™è®Šæ•¸ï¼šä¸€å¹´å¾Œç—…æƒ…ç™¼å±•ç‹€æ³
- `data.keys()`ï¼šé¡¯ç¤ºæ‰€æœ‰çš„keyå€¼

å°‡è³‡æ–™å–å‡ºæ”¾å…¥dataframe
```python
import pandas as pd

feature = pd.DataFrame(data['data'], columns = data['feature_names'])
target = pd.DataFrame(data['target'], columns = ['target'])
df = pd.concat([feature, target], axis = 1)
```
- `pd.DataFrame(data, columns)`ï¼šå»ºç«‹dataframe
    - dataï¼šæ”¾å…¥dataframeçš„è³‡æ–™
    - columnsï¼šæ¬„ä½åç¨±ï¼ˆè¡Œï¼‰
      > å¯ä½¿ç”¨listå¤šæ¬„çµ¦äºˆï¼Œä¹Ÿå¯è‡ªè¡ŒæŒ‡å®š
        - è‡ªè¡ŒæŒ‡å®šï¼šlistæ ¼å¼
          ```python
          columns = ['target']
          ```

- `pd.concat(objs, axis, join, join_axes, ignore_index)`ï¼šåˆä½µdataframe
    - objsï¼šåˆä½µå°è±¡
      > listæ ¼å¼
    - axisï¼šåˆä½µæ–¹å‘
      - axis=0ï¼šæŒ‰rowæ–¹å‘åˆä½µï¼Œç›´å‘åˆä½µï¼ˆä¸Šä¸‹ï¼‰
      - axis=1ï¼šæŒ‰columnæ–¹å‘åˆä½µï¼Œæ©«å‘åˆä½µï¼ˆå·¦å³ï¼‰
    - joinï¼šé€£æ¥æ–¹å¼ï¼Œæœ‰å…©ç¨®æ¨¡å¼
      - outerï¼šé è¨­ï¼Œç›´æ¥å°‡ç©ºçš„è³‡æ–™ï¼Œç”¨NaNå¡«è£œ
      - innerï¼šç›´æ¥å°‡ç©ºçš„è³‡æ–™åˆªé™¤
    - join_axesï¼šæ©«å‘åˆä½µæ™‚ï¼ŒæŒ‡å®šindex
      > E.g. join_axes = \[df1.index]
    - ignore_indexï¼šåˆä½µæ™‚ï¼Œå¯å¿½ç•¥èˆŠçš„indexæ¬„ä½ï¼Œæ”¹ç”¨è‡ªå‹•ç”¢ç”Ÿçš„æ–°index
      > é è¨­True

å¤šè®Šé‡åœ–ï¼Œè§€å¯Ÿè®Šæ•¸ä¹‹é–“å½¼æ­¤çš„åˆ†ä½ˆé—œä¿‚
> æ•£ä½ˆåœ– + ç›´æ–¹åœ–
```python
import matplotlib.pyplot as plt
import seaborn as sns

cols = ['age', 'bmi','s1', 's5', 'target']
sns.pairplot(df[cols])
plt.tight_layout()

plt.savefig('scatterplot.png', dpi=300)
```
- `seaborn`ï¼šå¤šç¶­ç¹ªåœ–çš„å¥—ä»¶
- `sns.pairplot(è®Šæ•¸, hue, palette)`ï¼šå¤šè®Šé‡åœ–ï¼Œç¹ªè£½æˆå°é—œä¿‚
   > å°è§’ç·šï¼šç›´æ–¹åœ–
    - hueï¼šåˆ†ç¾¤ï¼Œé€²ä¸€æ­¥å€åˆ†ä¸åŒè®Šæ•¸
    - paletteï¼šé¡è‰²ä¸»é¡Œ
- `plt.tight_layout()`ï¼šé¿å…åœ–é‡ç–Šï¼Œå°‡å…¶åˆ†é–‹
- `plt.savefig(åœ–ç‰‡åç¨±, è§£æåº¦)`ï¼šå„²å­˜åœ–æª”

ç†±åº¦åœ–ï¼ˆHeat mapï¼‰ï¼Œç•«å‡ºç›¸é—œä¿‚æ•¸çŸ©é™£ï¼ˆcorrelation matrixï¼‰
```python
import numpy as np

cm = np.corrcoef(df[cols].values.T) 
sns.set(font_scale=1.5)
hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size':15}, yticklabels=cols, xticklabels=cols)

plt.tight_layout()
plt.savefig('correlation.png', dpi=300)
```
> ç›¸é—œæ€§æ„ˆä½ï¼Œé¡è‰²æ„ˆæ·±

- `dataframe.values`ï¼šå°‡dataframeè½‰ç‚ºarray
- `array.T`ï¼šè½‰ç½®ï¼Œcolumnè®Šç‚ºrowï¼Œrowè®Šç‚ºcolumn
  > \[row, column]
  >> E.g. å°‡åŸå§‹è³‡æ–™ç”± 441x5 è½‰ç‚º 5x441
- `np.corrcoef()`ï¼šç›¸é—œä¿‚æ•¸ï¼Œè¿”å›è®Šæ•¸ä¹‹é–“ç›¸é—œé—œä¿‚å¯†åˆ‡ç¨‹åº¦çš„æŒ‡æ¨™
  > inputï¼š2-D array
  >> ä»¥ row ç‚ºè®Šé‡/æ¬„ä½ï¼Œcolumn ç‚ºæ¯å€‹è§€æ¸¬å€¼/æ¯ç­†è³‡æ–™ï¼ˆæ‰€ä»¥éœ€è¦**è½‰ç½®**ï¼‰
- `sns.set(font_scale)`ï¼šè¨­ç½®ç¾å­¸åƒæ•¸
  - font_scaleï¼šç¨ç«‹ç¸®æ”¾å­—é«”å…ƒç´ å¤§å°
- `sns.heatmap(data, cbar, annot, square, fmt, annot_kws, yticklabels, xticklabels)`ï¼šç†±åº¦åœ–ï¼Œç•«å‡ºç›¸é—œä¿‚æ•¸çŸ©é™£
  - dataï¼šå¯ç‚º array æˆ– dataframe
    > è‹¥ç‚ºdataframeï¼Œå…¶ rowï¼ˆindexï¼‰/ column æœƒåˆ†åˆ¥å°æ‡‰åˆ° heatmap çš„ column / row
  - cbarï¼šæ˜¯å¦åœ¨ heatmap æ¸¬é‚Šç¹ªè£½é¡è‰²åˆ»åº¦åœ–
    > é è¨­ç‚º True
  - annotï¼šæ˜¯å¦åœ¨æ¯å€‹æ–¹æ ¼å…§å¯«å…¥è³‡æ–™
    > é è¨­ç‚º False
    >> è‹¥dataç‚ºarrayï¼ˆçŸ©é™£ï¼‰ï¼Œæœƒå¡«å…¥å°æ‡‰ä½ç½®çš„è³‡æ–™
  - squareï¼šè¨­å®šæ–¹æ ¼ç‚ºæ–¹å½¢
    > é è¨­ç‚º Falseï¼ŒåŸç‚ºé•·æ–¹å½¢
  - fmtï¼šå­—ä¸²æ ¼å¼
    > .2fï¼šæµ®é»æ•¸ï¼Œåˆ°å°æ•¸é»å¾Œå…©ä½
  - annot_kwsï¼šæ–¹æ ¼ä¸Šæ•¸å­—çš„å¤§å°ã€é¡è‰²ã€å­—å‹
    > inputç‚º{ }
  - xticklabelsï¼šxè»¸æ¨™ç±¤åè¼¸å‡º
  - yticklabelsï¼šyè»¸æ¨™ç±¤åè¼¸å‡º
  
å°‡ data set åˆ†å‰²ç‚º training set èˆ‡ testing set 
```python
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split

X, y = load_diabetes().data, load_diabetes().target
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=8)
```
- `load_diabetes().data`ï¼šç³–å°¿ç—…è³‡æ–™é›†çš„ç‰¹å¾µè³‡æ–™é›†
- `load_diabetes().target`ï¼šç³–å°¿ç—…è³‡æ–™é›†çš„ç›®æ¨™è³‡æ–™é›†
- `train_test_split(data, target, random_state)`ï¼šå¾æ¨£æœ¬ä¸­éš¨æ©Ÿçš„æŒ‰æ¯”ä¾‹é¸å– train data èˆ‡ test data
    - dataï¼šè¦åˆ†å‰²çš„æ¨£æœ¬ç‰¹å¾µé›†ï¼ˆfeature datasetï¼‰
    - targetï¼šè¦åˆ†å‰²çš„ç›®æ¨™å‡½æ•¸é›†ï¼ˆtarget datasetï¼‰
    - random_stateï¼šéš¨æ©Ÿæ•¸çš„ç¨®å­

è¨“ç·´è¿´æ­¸æ¨¡å‹ï¼Œä¸¦è¨ˆç®— MSE è·Ÿ R<sup>2</sup>
> ç·šæ€§è¿´æ­¸ï¼Œç›®æ¨™è®Šæ•¸å¿…ç‚ºé€£çºŒå€¼
```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

slr = LinearRegression()

slr.fit(X_train, y_train)
y_train_pred = slr.predict(X_train)
y_test_pred = slr.predict(X_test)

print('MSE_train: %.3f, MSE_test: %.3f' % (mean_squared_error(y_train, y_train_pred), mean_squared_error(y_test, y_test_pred)))
print('R^2_train: %.3f, R^2_test: %.3f' % (r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)))
#è¼¸å‡º
MSE_train: 2812.369, MSE_test: 3108.041
R2_train: 0.530, R2_test: 0.459
```
> ç”± MSE èˆ‡ R<sup>2</sup>å¯ä»¥çœ‹å‡ºï¼Œæ­¤æ¨¡å‹ç™¼ç”Ÿäº† overfitting\
> å›  training data çš„ MSE ä½æ–¼ testing data\
>  å›  training data çš„ R<sup>2</sup> é«˜æ–¼ testing data

- y_train_predï¼šæ¨¡å‹é æ¸¬training dataçš„çµæœ
- y_test_predï¼šæ¨¡å‹é æ¸¬testing dataçš„çµæœ


åœ¨å¤šè®Šæ•¸çš„ç·šæ€§è¿´æ­¸ä¸­ï¼Œç·šæ€§è¿´æ­¸æ¨¡å‹è¦æ±‚è§£é‡‹èƒ½åŠ›ï¼Œä¹Ÿå°±æ˜¯é æ¸¬å› å­ï¼ˆç‰¹å¾µè®Šæ•¸
ï¼‰å°çµæœï¼ˆç›®æ¨™è®Šæ•¸ï¼‰ä¹‹é–“çš„å½±éŸ¿ï¼Œåœ¨è™•ç†æ™‚æœ‰ä»¥ä¸‹å¹¾é»è¦æ³¨æ„
> æ¨¡å‹è§£é‡‹èƒ½åŠ›ï¼šæ”¹å–„å“ªå€‹å› å­æ™‚ï¼Œå°çµæœæ˜¯æœ‰æ¯”è¼ƒå¤§çš„å¹«åŠ©çš„

- Note 1ï¼šXè¦å…ˆæ¨™æº–åŒ–
  > å»é™¤å–®ä½çš„å½±éŸ¿
- Note 2ï¼šè¿´æ­¸ä¿‚æ•¸æ•¸å€¼æ„ˆå¤§ï¼Œè¡¨ç¤ºå°yçš„å½±éŸ¿åŠ›æ„ˆå¤§
  > è¿´æ­¸ä¿‚æ•¸ï¼šæ¬Šé‡ï¼Œç•¶å…¶ä»–é æ¸¬å› å­å­˜åœ¨çš„æƒ…æ³ä¸‹ï¼Œè©²é æ¸¬å› å­çš„å¼·åº¦
  >> å‰æï¼Œè¦å°å¿ƒé æ¸¬å› å­ä¹‹é–“æ˜¯å¦æœƒäº’ç›¸å½±éŸ¿ï¼ˆç¢ºä¿å› å­æ˜¯ç›¸äº’ç¨ç«‹çš„ï¼‰ï¼Œå¦å‰‡æœƒç”¢ç”Ÿã€Œå…±ç·šæ€§å•é¡Œã€
- Note 3ï¼šè¿´æ­¸ä¿‚æ•¸ç‚ºè² ï¼Œè¡¨ç¤ºè² ç›¸é—œ

é—œæ–¼ã€Œå…±ç·šæ€§å•é¡Œã€ï¼Œå…¶ç™¼ç”Ÿçš„åŸå› ç‚ºè¿´æ­¸ä¿‚æ•¸ä¸­çš„è®Šæ•¸å½¼æ­¤ä¹‹é–“æœ‰ç›¸é—œé—œä¿‚ï¼Œå› è€Œå°è‡´è¿´æ­¸ä¿‚æ•¸è¢«æ‰­æ›²
 > E.g. "æ­£ç›¸é—œ"è®Šç‚º"è² ç›¸é—œ"

- è™•ç†æ–¹å¼
  - è³‡æ–™é è™•ç†ï¼šè§£æ±ºå› å­ä¹‹é–“çš„ç›¸äº’é—œä¿‚
      - è³‡æ–™è½‰æ›
        > E.g. å–å…©è®Šæ•¸çš„å¹³å‡å€¼ â†’ ç”¢ç”Ÿå¦ä¸€å€‹è®Šæ•¸
      - åªç•™ä¸‹ç¨ç«‹ï¼ˆä¸ç›¸é—œï¼‰è®Šæ•¸
  - è„Šè¿´æ­¸ï¼ˆridge regressionï¼‰
  - ä¸»æˆåˆ†åˆ†æï¼ˆprincipal component analysisï¼‰ï¼šPCAï¼Œç•™ä¸‹ä¸»è¦æˆä»½ï¼ˆé™ç¶­ã€ç¶­åº¦ç¸®æ¸›ï¼‰

[ğŸ”­](https://github.com/vanikk06/Machine-Learning/tree/master/Regression#content)  
