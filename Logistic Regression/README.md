# Content
- [Lecture 4 Logistic Regression æ‹·è².pdf](https://github.com/vanikk06/Machine-Learning/blob/master/Logistic%20Regression/Lecture%204%20Logistic%20Regression%20%E6%8B%B7%E8%B2%9D.pdf)
- [Logistic Regression](https://github.com/vanikk06/Machine-Learning/tree/master/Logistic%20Regression#logistic-regression)
- [Sigmoid function](https://github.com/vanikk06/Machine-Learning/tree/master/Logistic%20Regression#sigmoid-function)
- [Decision Boundary](https://github.com/vanikk06/Machine-Learning/tree/master/Logistic%20Regression#decision-boundary)
- [Logistic regression's cost function](https://github.com/vanikk06/Machine-Learning/tree/master/Logistic%20Regression#logistic-regressions-cost-function)
- [Praticing logistic regression](https://github.com/vanikk06/Machine-Learning/tree/master/Logistic%20Regression#praticing-logistic-regression)


# Logistic Regression
  > é‚è¼¯å¼è¿´æ­¸

ç›£ç£å¼å­¸ç¿’ï¼Œè™•ç†ã€Œåˆ†é¡å•é¡Œã€ï¼Œå¤§å¤šç‚ºäºŒå…ƒåˆ†é¡

- outputï¼šåƒ…æœ‰å…©ç¨®é¸æ“‡
  - 1ï¼šTrue
  - 0ï¼šFalse
  
æ ¹æ“šç‰¹å¾µè®Šæ•¸ä¾†é æ¸¬çµæœçš„**æ©Ÿç‡**ï¼Œå³å¾dataä¸­çš„ä¸åŒè®Šæ•¸åˆ¤æ–·output
> çŸ¥é“ä¸€ç¨®æ©Ÿç‡å³å¯ï¼Œå› åƒ…æœ‰å…©ç¨®çµæœ

- modelï¼šå¾è®Šæ•¸Xä¸­ï¼Œå¯å¾—çŸ¥yæ˜¯å¦æ˜¯ç­‰æ–¼1çš„æ©Ÿç‡
  > yï¼šæ¢ä»¶æ©Ÿç‡ï¼Œå¿…ç•Œæ–¼\[0, 1]
  >> E.g. Xï¼šæ¿•åº¦ã€yï¼šæ˜¯å¦æœƒä¸‹é›¨
  
åœ¨ Linear regressionä¸­è™•ç†çš„æ˜¯ã€Œé€£çºŒå‹ã€çš„è³‡æ–™ï¼Œç„¶è€Œåœ¨Logistic regressionä¸­æ˜¯è¦è™•ç†ã€Œé¡åˆ¥å‹ã€çš„åˆ†é¡å•é¡Œ\
åœ¨Logistic regressionè£¡ï¼Œå¯å…ˆé€éLinear regression modelå°‡ç‰¹å¾µè®Šæ•¸èˆ‡outputä¹‹é–“çš„é—œä¿‚è¡¨ç¾å‡ºä¾†ï¼ˆç®—å‡ºå€¼ï¼‰ï¼Œå†é€éã€Œfunctionè½‰æ›ã€çš„æ–¹å¼ï¼Œå°‡Linear regression modelæ‰€å¾—å‡ºçš„é€£çºŒå‹çµæœè½‰æ›ç‚ºæ©Ÿç‡
  - Linear regression modelï¼šè¨ˆç®—è®Šæ•¸Xèˆ‡outputä¹‹é–“é—œä¿‚
  - functionè½‰æ›ï¼šå°‡modelçµæœè½‰ç‚ºæ©Ÿç‡
    > E.g. Sigmoid function

[ğŸ¥ƒ](https://github.com/vanikk06/Machine-Learning/tree/master/Logistic%20Regression#content)

# Sigmoid function
  > è½‰æ›é€£çºŒå€¼ç‚ºæ©Ÿç‡çš„function
  
![](https://github.com/vanikk06/Machine-Learning/blob/master/Logistic%20Regression/image/Snipaste_2020-02-20_03-04-40.png) 
![](https://github.com/vanikk06/Machine-Learning/blob/master/Logistic%20Regression/image/Snipaste_2020-02-20_03-05-10.png)
> exp()ï¼šæ¬¡æ–¹çš„éƒ¨ä»½
  - ç•¶ z å¾ˆå¤§æ™‚ï¼Œe<sup>-z</sup>æœƒå¾ˆå° â†’ æ¥µåº¦é è¿‘1
    > å¿…ç‚ºæ­£æ•¸ï¼Œä¸”ä¸å¯èƒ½è¶…é1
  - ç•¶ z å¾ˆå°æ™‚ï¼Œe<sup>-z</sup>æœƒå¾ˆå¤§ â†’ è¶¨è¿‘æ–¼0
  - ç•¶ z = 0 æ™‚ï¼Œe<sup>-z</sup> = 1 â†’ 1/2ï¼ˆå³0.5ï¼‰

#### Pratices
å…ˆåŒ¯å…¥ç›¸é—œå¥—ä»¶
```python
import matplotlib.pyplot as plt
import numpy as np
import math
```
å¸¶å…¥å‡½å¼ç•«åœ–
```python
y = [1 / (1+math.exp(-x)) for x in np.linspace(-6, 6, 100)]

plt.plot(y)
```
- `math.exp()`ï¼šæ•¸å­¸å‡½å¼æŒ‡æ•¸é …

[ğŸ¥¤](https://github.com/vanikk06/Machine-Learning/tree/master/Logistic%20Regression#content)


# Decision Boundary
  > æ±ºç­–é‚Šç•Œ
  
ç®—å‡ºæ©Ÿç‡å€¼å¾Œï¼Œè¦é€²è¡Œåˆ†é¡çš„åˆ¤æ–·
> ç‚ºã€Œæ¢ä»¶æ©Ÿç‡ã€ï¼šåœ¨xç™¼ç”Ÿçš„æ¢ä»¶ä¸‹ï¼Œç™¼ç”Ÿoutputçš„æ©Ÿç‡

![](https://github.com/vanikk06/Machine-Learning/blob/master/Logistic%20Regression/image/Snipaste_2020-02-20_03-32-01.png)

æ±ºç­–é‚Šç•Œï¼Œä¸€æ¢åœ¨ç©ºé–“ä¸­çš„ç·šï¼Œå¯ä½œç‚ºé‚Šç•Œï¼Œå‘Šè¨´æˆ‘å€‘è®Šæ•¸è½åœ¨å„é‚Šç•Œæ‡‰å¾—çš„çµæœç‚ºä½•\
ç‚ºäº†è™•ç†ç·šæ€§ä¸å¯åˆ†é›¢çš„dataï¼Œæ±ºç­–é‚Šç•Œä¹Ÿå¯ä»¥æ˜¯ä¸€æ¢å¤šé …å¼çš„ç·šï¼Œä»¥æ­¤è™•ç†éç·šæ€§çš„æ±ºç­–é‚Šç•Œ


[ğŸ¥‚](https://github.com/vanikk06/Machine-Learning/tree/master/Logistic%20Regression#content)

# Logistic regression's cost function
  > Logistic regressionçš„ç›®æ¨™å‡½å¼
  
è‹¥è³‡æ–™ç‚º y=1 ï¼Œå¸Œæœ›åœ¨xçš„æƒ…æ³ä¸‹ï¼Œé æ¸¬çµæœç‚º y=1 çš„æ©Ÿç‡æ„ˆé«˜æ„ˆå¥½\
è‹¥è³‡æ–™ç‚º y=0 ï¼Œå¸Œæœ›åœ¨xçš„æƒ…æ³ä¸‹ï¼Œé æ¸¬çµæœç‚º y=0 çš„æ©Ÿç‡æ„ˆé«˜æ„ˆå¥½

å› é æ¸¬çµæœç‚ºæ©Ÿç‡ï¼Œå¸Œæœ›å„ç­†è³‡æ–™çš„çµæœåŒæ™‚æˆç«‹æ™‚ï¼Œé ˆå°‡æ‰€æœ‰ç›¸ä¹˜ï¼Œå¸Œæœ›æœ€çµ‚çš„ç›®æ¨™æ„ˆå¤§æ„ˆå¥½

![](https://github.com/vanikk06/Machine-Learning/blob/master/Logistic%20Regression/image/Snipaste_2020-02-21_03-17-30.png)
> æ©Ÿç‡ç›¸ä¹˜ä¸€èˆ¬åŒ–çš„å¼å­

- è‹¥ y<sup>(i)</sup> = 1ï¼Œåƒ…ç•™ä¸‹å‰é …ï¼Œå¾Œé …å› 0æ¬¡æ–¹è€Œè®Šç‚º1
- è‹¥ y<sup>(i)</sup> = 0ï¼Œåƒ…ç•™ä¸‹å¾Œé …ï¼Œå‰é …å› 0æ¬¡æ–¹è€Œè®Šç‚º0

ä½†å› æ©Ÿç‡ä»‹æ–¼\[0, 1]ä¹‹é–“ï¼Œå…¨éƒ¨ç›¸ä¹˜æœƒæ„ˆä¹˜æ„ˆå°ï¼Œæ…¢æ…¢è¶¨è¿‘æ–¼0ï¼Œå°è‡´éå°ç„¡æ³•é€ æˆå½±éŸ¿ï¼Œå› æ­¤å°‡æœ€çµ‚ç›®æ¨™å–logï¼Œä½¿å€¼æ”¾å¤§

![](https://github.com/vanikk06/Machine-Learning/blob/master/Logistic%20Regression/image/Snipaste_2020-02-21_03-18-16.png)

æœ€å¾Œåœ¨é…åˆregression ä¸€èˆ¬cost functionç›®æ¨™å€¼ç‚ºæ„ˆå°æ„ˆå¥½ï¼ŒåŠ ä¸Šã€Œè² è™Ÿã€

![](https://github.com/vanikk06/Machine-Learning/blob/master/Logistic%20Regression/image/Snipaste_2020-02-21_03-19-28.png)
> å› æ­¤ï¼Œæœ€çµ‚ç‚ºå¸Œæœ›ç›®æ¨™å‡½æ•¸ã€Œæ„ˆå°æ„ˆå¥½ã€

é™¤æ­¤ä¹‹å¤–ï¼Œé‚„å¸Œæœ›modelæ„ˆç°¡å–®æ„ˆå¥½

![](https://github.com/vanikk06/Machine-Learning/blob/master/Logistic%20Regression/image/Snipaste_2020-02-21_03-20-05.png)
> ç´…è‰²éƒ¨åˆ†ç‚ºæ§åˆ¶modelè¤‡é›œåº¦
>> Î»ï¼šæ­£è¦åŒ–åƒæ•¸ï¼ˆregularization parameterï¼‰ï¼Œæ§åˆ¶ã€Œcost functionã€èˆ‡ã€Œè¤‡é›œåº¦ã€çš„æ¯”é‡

[ğŸ»](https://github.com/vanikk06/Machine-Learning/tree/master/Logistic%20Regression#content)
  
# Praticing logistic regression  
  > ä½¿ç”¨ é³¶å°¾èŠ±æ•¸æ“šé›†

logistic regression æœ¬ç‚ºè™•ç†äºŒå…ƒåˆ†é¡ï¼Œç„¶è€Œå¾äºŒå…ƒåˆ†é¡è®Šå½¢ï¼Œä»å¯è™•ç†å¤šå…ƒåˆ†é¡çš„çš„å•é¡Œ

- è®Šå½¢æ–¹æ³•

  å‡è¨­ç›®æ¨™å‡½æ•¸æœ‰ä¸‰å€‹å€¼ï¼šç´…è‰²ã€è—è‰²èˆ‡ç¶ è‰²
  
  é¦–å…ˆè¨“ç·´ç´…è‰²è³‡æ–™çš„æ±ºç­–é‚Šç•Œï¼šå€åˆ†ã€Œç´…è‰²ã€èˆ‡ã€Œéç´…è‰²ã€çš„éƒ¨åˆ†ï¼Œé‡è¤‡æ­¤æ–¹å¼ï¼Œè¨“ç·´è—è‰²èˆ‡ç¶ è‰²çš„æ±ºç­–é‚Šç•Œ
  
  åœ¨æ–°çš„è³‡æ–™é€²å…¥æ™‚ï¼Œæœƒä¸€å€‹å€‹æ¯”è¼ƒï¼Œåˆ¤æ–·å±¬æ–¼ä½•é¡è‰²
  
  - è‹¥new_dataä»‹æ–¼å…©æ±ºç­–å€åŸŸä¹‹é–“ï¼šæ¯”è¼ƒæ©Ÿç‡å¼·åº¦/åŠ›åº¦ï¼ˆè·é›¢ï¼‰ï¼Œèˆ‡æ±ºç­–é‚Šç•Œçš„è·é›¢æ„ˆé ï¼Œè¡¨ç¤ºåˆ†éšŠçš„ä¿¡å¿ƒå¼·åº¦æ„ˆé«˜
    > æ¶è‘—è¦
  - è‹¥new_dataè¢«æ’é™¤åœ¨æ±ºç­–å€åŸŸä¹‹é–“ï¼šæ¯”è¼ƒæ©Ÿç‡å¼·åº¦/åŠ›åº¦ï¼ˆè·é›¢ï¼‰ï¼Œèˆ‡æ±ºç­–é‚Šç•Œçš„è·é›¢æ„ˆè¿‘ï¼Œè¡¨ç¤ºåˆ¤çµ¦å…¶çš„æ©Ÿç‡æ„ˆé«˜
    > å¤§å®¶éƒ½ä¸è¦


å…ˆåŒ¯å…¥è³‡æ–™é›†èˆ‡ç›¸é—œå¥—ä»¶
```python
import pandas as pd
import numpy as np
from sklearn.datasets import load_iris

iris = load_iris()
iris.keys()
#è¼¸å‡º
dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])
```
å°‡è³‡æ–™è½‰ç‚ºdataframeå½¢å¼ï¼Œä¸¦å€åˆ†ã€Œ2ã€èˆ‡ã€Œé2ã€çš„éƒ¨åˆ†ï¼ˆåƒ…å–å…©å€‹é¡åˆ¥ï¼‰
```python
feature = pd.DataFrame(iris['data'], columns = iris['feature_names'])
target = pd.DataFrame(iris['target'], columns = ['class'])

data = pd.concat([feature, target], axis=1)

df = data[data['class'] != 2]
```
ç•«å‡ºæ•£ä½ˆåœ–ï¼ŒæŸ¥çœ‹è³‡æ–™å‹æ…‹ï¼Œæ˜¯å¦å¯ç”¨ä¸€æ¢ç·šå€åˆ†
```python
import matplotlib.pyplot as plt
import seaborn as sns

plt.style.use('ggplot')
g = sns.FacetGrid(df, hue='class', size=5)
g.map(plt.scatter, "sepal length (cm)", "sepal width (cm)")
g.add_legend()
```
- `plt.style.use()`ï¼šå„ªåŒ–è¦–è¦ºåŒ–
- `sns.FacetGrid(data, row, col, hue, size)`ï¼šåˆå§‹åŒ–ç¶²æ ¼ï¼Œè¨­ç½®ç•«æ¿å’Œè»¸ï¼Œä½†ç„¡ç¹ªè£½ç•«æ¿ä¸Šä»»ä½•æ±è¥¿
    - dataï¼šè³‡æ–™å‹æ…‹ç‚º DataFrame
    - rowã€colï¼šå¯æ”¾å…¥æŒ‡å®šæ¬„ä½ï¼Œèˆ‡ã€Œè»¸ã€æœ‰æ˜é¡¯å°æ‡‰é—œä¿‚
    - hueï¼šæ ¹æ“šæ­¤æŒ‡å®šæ¬„ä½é€²è¡Œåˆ†ç¾¤ï¼Œä½¿ç”¨ä¸åŒé¡è‰²
    - sizeï¼šå¤§å°
- `.map(method, x, y)`ï¼šåœ¨ç¶²æ ¼ä¸Šç¹ªè£½æ•¸æ“š
    - methodï¼šç¹ªè£½æ–¹æ³•
        - plt.scatterï¼šæ•£ä½ˆåœ–
        - plt.histï¼šç›´æ–¹åœ–
        - sns.regplotï¼šæ•£ä½ˆåœ–ä¹‹ä¸€
        - sns.barplotï¼šé•·æ¢åœ–
        - sns.boxplotï¼šç›’é¬šåœ–
    - xï¼šxè»¸æ¨™ç±¤åç¨±
    - yï¼šyè»¸æ¨™ç±¤åç¨±
- `.add_legend()`ï¼šå¢åŠ åœ–ä¾‹

å…ˆå°‡è³‡æ–™æ¨™æº–åŒ–ï¼Œä»¥å¢åŠ æ”¶æ–‚é€Ÿåº¦ï¼Œé¿å…åå‘æŸå€‹è®Šæ•¸è¨“ç·´
```python
from sklearn.preprocessing import StandardScaler
X = df.iloc[:, :2].values 
y = df.iloc[:, 4].values

sc = StandardScaler()
sc.fit(X)
X_std = sc.transform(X)
```
> Xï¼šåƒ…å–å‰å…©å€‹ç‰¹å¾µè®Šæ•¸

è¨“ç·´ logistic regression model
> ä½¿ç”¨`sklearn.linear_model`å¥—ä»¶çš„`LogisticRegression`
```python
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(C=100.0, random_state=1)
lr.fit(X_std, y)

print(lr.coef_)
print(lr.intercept_)
#è¼¸å‡º
[[ 9.87113623 -6.67376595]]
[1.7401565]
```
> è¼¸å‡ºæ±ºç­–é‚Šç•Œï¼ˆç·šï¼‰

- `LogisticRegression(C, random_state)`ï¼šlogistic regression model
    - Cï¼š1/Î»ï¼Œæ­£å‰‡å¼·åº¦çš„å€’æ•¸ï¼Œå€¼æ„ˆå°æ­£å‰‡å¼·åº¦å°±æ„ˆå¼·
      > æµ®é»æ•¸

å®šç¾©æ±ºç­–å€åŸŸï¼ˆdecision regionï¼‰
```python
from matplotlib.colors import ListedColormap

def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):

    #setup marker generator and color mapè¨­ç½®æ¨™è¨˜ç”Ÿæˆå™¨ & é¡è‰²åœ–
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
    
    #plot the decision surfaceç¹ªè£½æ±ºç­–é¢
    x1_min, x1_max = X[:, 0].min()-1, X[:, 0].max()+1
    x2_min, x2_max = X[:, 1].min()-1, X[:, 1].max()+1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)    
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    
    #ç•«å‡ºè³‡æ–™é»(yï¼šå¯¦éš›é»)
    for idx, cl in enumerate(np.unique(y)):
      plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, c=colors[idx], marker=markers[idx], label=cl, edgecolor='black')
      
    #highlight test samples
    if test_idx:
        #plot all samples
        X_test, y_test = X[test_idx, :], y[test_idx]
        
        plt.scatter(X_test[:, 0], X_test[:, 1], c='', edgecolor='black', alpha=1.0, linewidth=1, marker='o', s=100, label='test set')
```
- `ListedColormap`ï¼šå‰µé€ æˆ–æ˜¯æ“ä½œé¡è‰²æ˜ å°„
- classifierï¼šè¿´æ­¸æ¨¡å‹
- resolutionï¼šè§£æåº¦(é–“éš”)
- `ListedColormap(colors[:len(np.unique(y))])`ï¼šæ‰¾å‡ºy(ç›®æ¨™è®Šæ•¸)ç¨®é¡ï¼Œç”¨colorå…§é¡è‰²å€åˆ†(æœ€å¤š5é¡)
   - `np.unique(input, return_index=True, return_inverse=True)`ï¼šå»é™¤é‡è¤‡çš„å…ƒç´ ï¼Œä¸¦æŒ‰å…ƒç´ é †åºç”±å°åˆ°å¤§æ’åˆ—ï¼Œè¿”å›ä¸€å€‹æ–°çš„ç„¡é‡tuple or list
     - return_index=Trueï¼šè¿”å›new_listå…ƒç´ åœ¨old_listä¸­çš„ä½ç½®ï¼Œä¸¦ä»¥listå½¢å¼å¦å¤–å„²å­˜
       > è¦å¤šçµ¦ä¸€å€‹ç©ºé–“ï¼Œé•·åº¦ç­‰æ–¼new_list
     - return_inverse=Trueï¼šè¿”å›old_listå…ƒç´ åœ¨new_listä¸­çš„ä½ç½®ï¼Œä¸¦ä»¥listå½¢å¼å¦å¤–å„²å­˜
       > è¦å¤šçµ¦ä¸€å€‹ç©ºé–“ï¼Œé•·åº¦ç­‰æ–¼old_list
   - `colors[:x]`ï¼šå¾index=0åˆ°index=x-1(ç¸½å…±æœƒå‡ºç¾xå€‹å…ƒç´ )

- x1_minã€x1_maxï¼šæ‰¾å‡ºç¬¬ä¸€æ¬„çš„ç¯„åœ(min-1, Max+1)
- x2_minã€x2_maxï¼šæ‰¾å‡ºç¬¬äºŒæ¬„çš„ç¯„åœ(min-1, Max+1)
- xx1ã€xx2ï¼šç”¨Xè®Šæ•¸æ‰¾å‡ºxè»¸ã€yè»¸
   - `np.arange(start, end, step)`ï¼šå»ºç«‹ä¸€å€‹rangeå…§çš„list
   - `np.meshgrid(x, y)`ï¼šç”¨å…©å€‹åº§æ¨™è»¸ä¸Šçš„é»åœ¨å¹³é¢ä¸Šç•«ç¶²æ ¼
- Zï¼šä½¿ç”¨çµ¦å®šçš„å¥—ä»¶æ¨¡å‹å»é æ¸¬(æ±ºç­–é‚Šç·š)
   - `.ravel()`ï¼šé™ç‚º1ç¶­
   - `np.array`ï¼šå»ºç«‹ä¸€å€‹arrayï¼Œåˆä½µxx1ã€xx2è³‡æ–™
   - `.T`ï¼šè½‰ç½®
- `plt.contourf(data_x, data_y, z, alpha, camp)`ï¼šç¹ªè£½è¼ªå»“ç·šä¸”å¡«å……è¼ªå»“
  - data_xï¼šXè»¸æ•¸æ“š
  - data_yï¼šYè»¸æ•¸æ“š
  - zï¼šæŒ‡å®šXã€Yåº§æ¨™å°æ‡‰é»çš„é«˜åº¦æ•¸æ“š
  - alphaï¼šé€æ˜åº¦
  - cmapï¼šæŒ‡å®šç­‰é«˜ç·šçš„é¡è‰²æ˜ å°„(è‡ªå‹•ç”¨ä¸åŒçš„é¡è‰²ä¾†å€åˆ†ä¸åŒçš„é«˜åº¦å€åŸŸ)

ç•«å‡ºç›®å‰modelçš„æ±ºç­–é‚Šç•Œ
```python
plot_decision_regions(X_std, y, classifier=lr)
plt.xlabel('sepal length [standardized]')
plt.ylabel('sepal width [standardized]')
plt.legend(loc='upper left')
plt.tight_layout()
```

#### Source
[Seabornå­¦ä¹ ï¼ˆä¸€ï¼‰------- æ„å»ºç»“æ„åŒ–å¤šç»˜å›¾ç½‘æ ¼ï¼ˆFacetGrid(ï¼‰ã€map()ï¼‰è¯¦è§£](https://blog.csdn.net/weixin_42398658/article/details/82960379)

[18-12-11-è¦–è¦ºåŒ–åº«Seabornå­¸ç¿’ç­†è¨˜ï¼ˆå…­ï¼šFacetGridï¼‰](https://www.itread01.com/content/1545030424.html)

[machine learning ä¸‹çš„ Logistic Regression å¯¦ä½œ(ä½¿ç”¨python)](https://medium.com/@jacky308082/machine-learning-%E4%B8%8B%E7%9A%84-logistic-regression-%E5%AF%A6%E4%BD%9C-%E4%BD%BF%E7%94%A8python-d19b971ff9dc)

[Pythonä¸­numpyåº«uniqueå‡½æ•¸è§£æ](https://blog.csdn.net/yangyuwen_yang/article/details/79193770)

[Numpyä¸­Meshgridå‡½æ•¸ä»‹ç´¹åŠ2ç¨®æ‡‰ç”¨å ´æ™¯](https://www.cnblogs.com/lemonbit/p/7593898.html#4184208)

[Python Matplotlib contourå’Œcontourfï¼šç¹ªè£½ç­‰é«˜ç·š](http://c.biancheng.net/view/2719.html)

[matplotlib.pyplot contourf()å‡½å¼çš„ä½¿ç”¨](https://www.itread01.com/content/1542579970.html)

[ğŸº](https://github.com/vanikk06/Machine-Learning/tree/master/Logistic%20Regression#content)
